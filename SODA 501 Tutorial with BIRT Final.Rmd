---
title: "Week 4 Tutorial: Reliability and Item Response Theory with brms"
subtitle: "SODA 501"
author: "Sharon Kim"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    toc_depth: 3
    number_sections: yes
    code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
editor_options:
  chunk_output_type: console
---


```{r hello world, echo = F}
pacman::p_load(multicolor, crayon, cowsay)

say("Let's model with Stan!", "cat", 
    what_color = "#8F64F3", by_color = "#000000")
```

# Overview

Keeping with this week's theme of measurement, we will be evaluating the internal reliability of measurements using Cronbach's alpha coefficient in R and doing a bit of Bayesian Item Response Theory with brms.

To give a bit of background about brms and Stan. Stan is a software that uses probabilistic programming language written in C++ for modeling and statistical computing. Stan is well-suited to handle large datasets, wide range of models, and complex posterior distributions accurately (and quickly). Bayesian regression modeling in Stan (i.e. brms) is an R package that helps interface with Stan to be more user friendly. If interested in learning more about brms or Stan, I highly recommend checking out their website for documentation and various helpful online resources. This is another great tool to add to your social computational tool kit.

<hr>

# Outline

In this tutorial, we will cover:  

- A. Install and set-up  
- B. Data processing  
- C. Reliability  
- D. Item Response Theory

<hr>

# A. Install and Set-Up

## Step 0: Install Stan

**Links to Stan Resources:**  
- [Stan Interfaces](https://mc-stan.org/users/interfaces/)  
- [RStan](https://mc-stan.org/users/interfaces/rstan.html)  
- [RStan Getting Started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started) - [brms](https://paul-buerkner.github.io/brms/)



Before install RStan, we must configure C++ Toolchain to be able to compile C++ code.  

**Links to installations:**  
- [Windows](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows)  
- [Mac](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Mac)  
- [Source](https://github.com/stan-dev/rstan/wiki/Installing-RStan-from-Source#mac)  

### Install on mac OS 

1. Configure C++ Toolchain  
- Installer package (XCode command line tools + gfortran)  

*Some resources if encounter issues:  *
- [Catalina Problems](https://github.com/stan-dev/rstan/wiki/Catalina-problems)  
- [XCode/GFortran](https://mac.r-project.org/tools/)  

2. Install RStan in R and restart R  

```{r install rstan on mac, eval = F}
# run the next line if you already have rstan installed
# remove.packages(c("StanHeaders", "rstan"))

# This is line you run to install rstan package
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```

### Install on windows OS

1. Configure C++ Toolchain
- Install [RTools42](https://cran.r-project.org/bin/windows/Rtools/rtools42/rtools.html)  
- ```{r, eval = F} install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))```  
- ```{r, eval = F} install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))```  
2. Install RStan in R and restart R  

```{r install rstan on windows, eval = F}
# run the next line if you already have rstan installed
# remove.packages(c("StanHeaders", "rstan"))

install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```

## Test run RStan

```{r test run rstan, eval = F}
library(rstan)
example(stan_model, package = "rstan", run.dontrun = TRUE)


# If model does not compile, then run the following lines that are commented out. We are reinstalling rstan and StanHeaders from source. 

# Compile packages using all cores
#Sys.setenv(MAKEFLAGS = paste0("-j",parallel::detectCores()))

#install.packages(c("StanHeaders","rstan"),type="source")
```

<hr>

# B. Data processing

## Step 0: Set-up R

### Clean slate

The following code chunk will clear your working environment so we are starting with a fresh slate. This is especially helpful when you are working on multiple scripts and/or are knitting the document. You can toggle the evaluation setting on (TRUE) or off (FALSE). It is currently set to false for your personal workflow.  

```{r clear environment, eval = FALSE}
# Closes graphics windows
#graphics.off()
# Clears the environment
#rm(list=ls())
```

### Load packages and Stan settings
```{r load packages}
# This is the package management tool package

#install.packages('pacman')
library(pacman)
p_load(rstan, coda, ggplot2, dplyr, psych, shinystan, bayesplot,
       lpSolve, psychTools, graphics, visdat, naniar, 
       car, sjPlot, GGally, tidyr, tableone,
       ltm, brms, cmdstanr)

## Stan options
#rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()) # Stan option to run MCMC chain in parallel

```


### Import data and dataframe set-up

When assessing the reliability of measures, it is important to explore the dataset and familiarize oneself with the measured variables.

```{r data set-up}
# Read in csv file
fullData = read.csv("emaDataForTutorial.csv", na.strings = "NA")


# Some quick descriptives of data and dataframe characteristics
describe(fullData)
str(fullData)
glimpse(fullData)
head(fullData)

# Number of participants
length(unique(fullData$PID))
```

#### Variable List

- **PID**: Participant ID from study (N = 160)  
- **DayOfStudy**: For this tutorial, only looking at measures on day 1. Participants were assessed multiple times within each day.  
- **Valence**: Positive to negative emotional valence (0 - 100)  
- **Arousal**: Intensity of emotional arousal (0 - 100)  
- **Relax**: How relaxed are you feeling in this moment (0 - 100)  
- **FeelingLoved**: How loved are you feeling in this moment (0 - 100)  
- **PERMA Well-being measure**: 3 items for each dimension (0 - 100). Indicate experience of well-being from these five dimensions of positive emotion, engagement, relationship, meaning, and accomplishment within this moment.<br>

### Compute new variables

First, here are simply some data processing scripts  

We are creating new variables: First, recoding PID from raw data identifier to one easier for analyses. Next, assessing how many trials each participant completed.  <br>

```{r compute new variables}
# Participants have their original PID string from the study. For simplicity, we will be recoding them from 1 and relabelling it from PID to ID
data <- fullData
dataID <- data %>%                     
  dplyr::group_by(PID) %>%
  dplyr::mutate(ID = cur_group_id())

# This is original participant ID list
PIndOG = unique(data$PID)
PIndRC = unique(dataID$ID)

# Here is just a reference table showing the original PID and the recoded ID
PIDTable <- cbind(PIndOG, PIndRC)

# Setting ID as a grouping variable
dataID = dataID %>% 
  dplyr::group_by(ID)

# Number of participants (160)
P = length(PIndRC)

# We are creating a list containing the number of trials for each participant
TperP = rep(NaN, P)
for (pp in 1:P){
  TperP[pp] = sum(dataID$ID == PIndRC[pp])
}

# Reference table showing original PID, recoded ID, and number of trials by each person
(pIDTable <- cbind(PIDTable, TperP))

# Explore missingness
vis_miss(dataID)
miss_case_table(dataID)
miss_var_table(dataID)

```

<hr>

# C. Reliability

We will assess the internal consistency of this PERMA measure to assess how well each item measures each dimensions of well-being. We will investigate the intercorrelations among each of the items tapping into each of the 5 dimensions.  

Important note about the data, this data has a nested structure that can be accounted for by using hierarchical modeling approaches and assessing reliability. For tutorial purposes, we will carry on.  

## P for positive emotionality

```{r P reliability}
# Creating subsets of just the 3 items measuring Positive Emotionality (P)
d <- dataID
sdf <- d[, c("Joyful", "Positive", "Contented")]

str(sdf)
describe(sdf)
summary(sdf)

# Places in vector of weird rows that are equal to 0
(weird <- which(sdf == 0))

# Provides frequency table
sapply(sdf, table, useNA='always')

# Provides frequency of each response grouped by item
sdfT <- as.data.frame(sdf)

# Sets graphical parameters
par(mfrow = c(1, 3)) 
sapply(sdf, hist) # histograms of each item

# Sets graphical parameters
par(mfrow = c(1, 1))
  # Correlation table
round(cor(sdf, use='pair'), 2)
  # Creates a correlation plot
image(x=1:3, y=1:3, z = cor(sdf)[,3:1])

sdfMat <- as.matrix(sapply(sdf, table, useNA='always'))
sdfMat <- na.omit(sdfMat)
sdfMat <- as.matrix(sapply(sdfMat, table, useNA='always'))

# Plots frequency of each response, grouped by item
dotchart(unlist(sapply(sdf, table, useNA = 'always'))) # With NA
dotchart(unlist(sapply(sdf, table))) # Without NA

# Explore missingness
vis_miss(sdf)
miss_case_table(sdf)
miss_var_table(sdf)

# Assess reliability (Cronbach's alpha coefficient)
psych::alpha(sdf)
(rel <- psych::alpha(sdf)$total)
```

**Reliability Analysis Results:**  
- We are looking at the estimated raw alpha coefficient of ```r round(rel[[1]], 2)```.  
- With a sample size of ```r nrow(sdf)```, it seems we have decent sample size to estimate narrow confidence intervals as well.  
- Raw alpha calculates the alpha coefficient from the item variance and total test variance, therefore is sensitive to heterogeneity in item variances.  
- Standardized alpha will standardize the item variability in computing reliability.  
- So each coefficient can be useful for different purposes. 
- The general rule of thumb for assessing reliability is (\alpha > .70).  
- We have sufficient internal consistency of our three items as reliably measuring positive emotionality.  
- *Therefore, we can aggregate across these 3 items.  * <br>

We apply this code to the other 4 dimensions.  

<hr>

## E for engagement

```{r E reliability}
# Creating subsets of just the 3 items measuring engagement (E)
d <- dataID
# ** Note: This script is same as from P. Simply change the items we are subsetting below:
sdf <- d[,paste0('Engagement', 1:3)]

str(sdf)
describe(sdf)
summary(sdf)

# Places in vector of weird rows that are equal to 0
(weird <- which(sdf == 0))

# Provides frequency table
sapply(sdf, table, useNA='always')

# Provides frequency of each response grouped by item
sdfT <- as.data.frame(sdf)

# Sets graphical parameters
par(mfrow = c(1, 3)) 
sapply(sdf, hist) # histograms of each item

# Sets graphical parameters
par(mfrow = c(1, 1))
  # Correlation table
round(cor(sdf, use='pair'), 2)
  # Creates a correlation plot
image(x=1:3, y=1:3, z = cor(sdf)[,3:1])

sdfMat <- as.matrix(sapply(sdf, table, useNA='always'))
sdfMat <- na.omit(sdfMat)
sdfMat <- as.matrix(sapply(sdfMat, table, useNA='always'))

# Plots frequency of each response, grouped by item
dotchart(unlist(sapply(sdf, table, useNA = 'always'))) # With NA
dotchart(unlist(sapply(sdf, table))) # Without NA

# Explore missingness
vis_miss(sdf)
miss_case_table(sdf)
miss_var_table(sdf)

# Assess reliability (Cronbach's alpha coefficient)
psych::alpha(sdf)
(rel <- psych::alpha(sdf)$total)
```

**Reliability Analysis Results:**  
- We are looking at the estimated raw alpha coefficient of ```r round(rel[[1]], 2)```.  
- With a sample size of ```r nrow(p)```, it seems we have decent sample size to estimate narrow confidence intervals as well.  
- Raw alpha calculates the alpha coefficient from the item variance and total test variance, therefore is sensitive to heterogeneity in item variances.  
- Standardized alpha will standardize the item variability in computing reliability.  
- So each coefficient can be useful for different purposes. 
- The general rule of thumb for assessing reliability is (\alpha > .70).  
- We have sufficient internal consistency of our three items as reliably measuring positive engagement. (This is close to the acceptable threshold, so just make note of this. Statistically, we can proceed.)
- *Therefore, we can aggregate across these 3 items.  * <br>

<hr>

## R for relationships

```{r R reliability}
# Creating subsets of just the 3 items measuring relationships (R)
d <- dataID
# ** Note: This script is same as from above. Simply change the items we are subsetting below:
sdf <- d[,paste0('Relationship', 1:3)]

str(sdf)
describe(sdf)
summary(sdf)

# Places in vector of weird rows that are equal to 0
(weird <- which(sdf == 0))

# Provides frequency table
sapply(sdf, table, useNA='always')

# Provides frequency of each response grouped by item
sdfT <- as.data.frame(sdf)

# Sets graphical parameters
par(mfrow = c(1, 3)) 
sapply(sdf, hist) # histograms of each item

# Sets graphical parameters
par(mfrow = c(1, 1))
  # Correlation table
round(cor(sdf, use='pair'), 2)
  # Creates a correlation plot
image(x=1:3, y=1:3, z = cor(sdf)[,3:1])

sdfMat <- as.matrix(sapply(sdf, table, useNA='always'))
sdfMat <- na.omit(sdfMat)
sdfMat <- as.matrix(sapply(sdfMat, table, useNA='always'))

# Plots frequency of each response, grouped by item
dotchart(unlist(sapply(sdf, table, useNA = 'always'))) # With NA
dotchart(unlist(sapply(sdf, table))) # Without NA

# Explore missingness
vis_miss(sdf)
miss_case_table(sdf)
miss_var_table(sdf)

# Assess reliability (Cronbach's alpha coefficient)
psych::alpha(sdf)
(rel <- psych::alpha(sdf)$total)
```

**Reliability Analysis Results:**  
- We are looking at the estimated raw alpha coefficient of ```r round(rel[[1]], 2)```.  
- With a sample size of ```r nrow(p)```, it seems we have decent sample size to estimate narrow confidence intervals as well.  
- Raw alpha calculates the alpha coefficient from the item variance and total test variance, therefore is sensitive to heterogeneity in item variances.  
- Standardized alpha will standardize the item variability in computing reliability.  
- So each coefficient can be useful for different purposes. 
- The general rule of thumb for assessing reliability is (\alpha > .70).  
- We have sufficient internal consistency of our three items as reliably measuring positive relationships.
- *Therefore, we can aggregate across these 3 items.  * <br>

<hr>

## M for relationships

```{r M reliability}
# Creating subsets of just the 3 items measuring meaning (M)
d <- dataID
# ** Note: This script is same as from above. Simply change the items we are subsetting below:
sdf <- d[,paste0('Meaning', 1:3)]

str(sdf)
describe(sdf)
summary(sdf)

# Places in vector of weird rows that are equal to 0
(weird <- which(sdf == 0))

# Provides frequency table
sapply(sdf, table, useNA='always')

# Provides frequency of each response grouped by item
sdfT <- as.data.frame(sdf)

# Sets graphical parameters
par(mfrow = c(1, 3)) 
sapply(sdf, hist) # histograms of each item

# Sets graphical parameters
par(mfrow = c(1, 1))
  # Correlation table
round(cor(sdf, use='pair'), 2)
  # Creates a correlation plot
image(x=1:3, y=1:3, z = cor(sdf)[,3:1])

sdfMat <- as.matrix(sapply(sdf, table, useNA='always'))
sdfMat <- na.omit(sdfMat)
sdfMat <- as.matrix(sapply(sdfMat, table, useNA='always'))

# Plots frequency of each response, grouped by item
dotchart(unlist(sapply(sdf, table, useNA = 'always'))) # With NA
dotchart(unlist(sapply(sdf, table))) # Without NA

# Explore missingness
vis_miss(sdf)
miss_case_table(sdf)
miss_var_table(sdf)

# Assess reliability (Cronbach's alpha coefficient)
psych::alpha(sdf)
(rel <- psych::alpha(sdf)$total)
```

**Reliability Analysis Results:**  
- We are looking at the estimated raw alpha coefficient of ```r round(rel[[1]], 2)```.  
- With a sample size of ```r nrow(p)```, it seems we have decent sample size to estimate narrow confidence intervals as well.  
- Raw alpha calculates the alpha coefficient from the item variance and total test variance, therefore is sensitive to heterogeneity in item variances.  
- Standardized alpha will standardize the item variability in computing reliability.  
- So each coefficient can be useful for different purposes. 
- The general rule of thumb for assessing reliability is (\alpha > .70).  
- We have sufficient internal consistency of our three items as reliably measuring meaning.
- *Therefore, we can aggregate across these 3 items.  * <br>

<hr>

## A: *Try it yourself!* for Accomplishment

I have left this blank for you to try it yourself. Same approach as above.

```{r Try it yourself: A reliability}

```

<hr>


## Create composite scores

After finishing the reliability analyses, we will create composite scores for our PERMA dimensions of well-being.  

```{r Stan data prep}
dataID$PositiveEm <- 
  rowMeans(dataID[,c("Joyful","Positive","Contented" )],na.rm = TRUE)
dataID$Engagement <- 
  rowMeans(dataID[,paste("Engagement", 
                1:3, sep="")],na.rm = TRUE)
dataID$Relationship <- 
  rowMeans(dataID[,paste("Relationship", 
                1:3, sep="")],na.rm = TRUE)
dataID$Meaning <- 
  rowMeans(dataID[,paste("Meaning", 
                1:3, sep="")],na.rm = TRUE)
dataID$Accomplishment <- 
  rowMeans(dataID[,paste("Accomplishment", 
                1:3, sep="")],na.rm = TRUE)

subDat <- select(dataID, c("ID", "FeelingLoved","PositiveEm", "Engagement", "Relationship", "Meaning", "Accomplishment"))

# Correlation table
round(cor(subDat[,3:7], use = 'complete'),2)

# Pairs plot
ggpairs(subDat)
```

<hr>

# D. Bayesian Item Response Theory

If we made it here, this means we had extra time, nice!

First, we are reading in the data. For this example, we will be starting with a simple example using binary models. This is a simulated dataset (N = 500) and in this hypothetical scenario, participants were scored as correct or incorrect on some item assessing the PERMA well-being dimensions.

```{r Data for IRT}
irtDat <- read.csv("irtDat500.csv", header = T)
colnames(irtDat) <- c("id", "Pos", "Eng", "Rel", "Mea", "Acc")
head(irtDat)
describe(irtDat)

# brms requires wide data
datWide = irtDat %>% 
  gather("Pos", "Eng", "Rel", "Mea", "Acc", key = item, value = resp)
head(datWide)
describe(datWide)

```

## 1PL in frequentist framework

```{r Frequentist 1PLM}
irtDat2 <- irtDat[,-1] # Remove ID
PL1.rasch <- rasch(irtDat2)
summary(PL1.rasch)

plot(PL1.rasch,type=c("ICC"))
plot(PL1.rasch,type=c("IIC"))
plot(PL1.rasch,type=c("IIC"),items=c(0))
item.fit(PL1.rasch,simulate.p.value=T)
theta.rasch<-ltm::factor.scores(PL1.rasch)
summary(theta.rasch$score.dat$z1)
plot(theta.rasch)
unidimTest(PL1.rasch,irtDat2)
```

Looking at the item difficulty estimates, we have significant z-values for all items except Relationship (z = 0.09). A z-value greater than 1.65 indicates that the difficulty parameter is significantly greater than zero at the alpha = 0.05 level. For example, difficulty of item assessing accomplishment is b = 2.59, z = 10.30. Higher difficulty estimates indicates that latent ability must be higher to have a 50% probability of getting the item "correct".

### brms and Stan

We can interface with Stan using the package brms.

```{r brms data set-up}
# Model formula
formula_1plm <- bf(resp ~ 0 + item + (1 | id))

# We can set priors like this. I have commented it out for now.
#priors <- get_prior(resp ~ 0 + item + (1 | id), data = datWide, family = bernoulli())

# Fitting the 1PL model
fit_1pl <- brm(formula = formula_1plm, data = datWide, family = brmsfamily("bernoulli", "logit"), backend = getOption("brms.backend", "cmdstanr"))


summary(fit_1pl)
```

#### Results

```{r trace plots}
# Plots mean parameter estimates and credible intervals
plot(fit_1pl)
```

Assess model convergence: Before interpreting results, we need to assess whether the model fitting algorithm converged. We can assess graphically via the trace plots (are chains mixing well and looking like fuzzy caterpillars), numerically via scale reduction factor R-hat (Should be close to 1 and less than 1.05), and the effective sample size (at least 400 and large as possible to ensure reliable convergence diagnostics).

Results: Looking at the posterior means and 95% credible intervals (CI), it looks like most items except for (Relationship) are agreed on by most individuals, while there is no credible agreement different from 0 for relationship (estimate = -0.01, 95%CI[-0.24, 0.21]). It looks like Engagement (estimate = -1.41, 95%CI[1.15, 1.68]) and Positive Emotionality (estimate = 3.69, 95%CI[3.21, 4.21]) are agreed upon to be positive and easy to discern. Whereas, accomplishment (estimate = -2.87, 95%CI[-3.25, -2.51]) and meaning (estimate = -1.62, 95%CI[-1.91, -1.36]) are agreed upon to be negative and difficult to discern.

### Assess model fit
- We can create traceplots to inspect model performance. Did our chains converge to a common distribution? This informs us that our chains are producing reliable point estimates of our parameters.  

- We can compare these parameter estimates with the multiple regression model we fit to the data earlier. Results from both look pretty similar!  

```{r frequentist vs bayesian}
# Bayesian 
print(fit_1pl)

# Frequentist
summary(PL1.rasch)

```

Thoughts? Comments? Questions?

```{r good bye world, echo = F}
cowsay::say("Thanks for your attention!", "shortcat", 
    what_color = "#8F64F3", by_color = "#000000")
```